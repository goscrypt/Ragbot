<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Offline Generative Document QA (Fixed Remote Loading)</title>
  <style>
    body { font-family: sans-serif; margin: 2em; }
    #chatLog { border: 1px solid #ccc; padding: 1em; height: 320px; overflow-y: auto; margin-bottom: 1em; white-space: pre-wrap; }
    .user { color: #0057d8; font-weight: bold; margin: 0.5em 0; }
    .bot { color: #157f1f; margin: 0.5em 0; }
    #statusMessage { margin: 0.5em 0; color: #444; }
    #warning { margin: 1em 0; padding: 1em; background: #ffe8e8; border: 1px solid #d33; color: #900; display: none; }
  </style>
</head>
<body>
  <h2>Offline Generative Document QA</h2>
  <p>Upload a .txt file or use the default demo doc. Models are fetched from Hugging Face on first use, then cached locally in your browser.</p>
  <div id="warning"></div>
  <input type="file" id="fileInput" accept=".txt"><br><br>
  <div id="fileInfo"></div>
  <div id="statusMessage">Loading models… please wait</div>
  <div id="chatLog">Chat history will appear here...</div>
  <input type="text" id="userInput" placeholder="Ask a question about your document..." disabled>
  <button id="sendBtn" disabled>Send</button>

  <script type="module">
    import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.1/dist/transformers.min.js';

    // --- Ensure we don't try local paths ---
    env.allowLocalModels = false;  // force remote Hugging Face CDN
    env.allowRemoteModels = true;  // always pull from hub if needed

    // --- Default Demo Document ---
    const defaultDoc = `
Why JavaScript for AI
- Works directly in the browser
- Runs across platforms
- Easy to use
- Great for lightweight AI demos
- Supports privacy-respecting workflows

Core Libraries
- Brain.js: Simplified, lightweight, supports feedforward and recurrent networks, best for beginners
- TensorFlow.js: Full-featured, complex, robust, supports many models, better for advanced users
- ml5.js: Simple APIs, built on TensorFlow.js, designed for education and demos

Building Blocks of AI in JavaScript
- Collect input (text, images, audio)
- Preprocess data (normalization, tokenization)
- Feed data into a model (pre-trained or custom)
- Interpret the output
- Trigger actions

Training vs Inference
- Training: creating models, resource-intensive, better in Python
- Inference: running trained models, lightweight
- JavaScript is best suited for inference
    `.trim();

    // --- Global State ---
    const state = {
      chunks: [],
      embeddings: [],
      fe: null,
      generator: null,
      isReady: false,
    };

    // --- DOM References ---
    const fileInput = document.getElementById('fileInput');
    const userInput = document.getElementById('userInput');
    const chatLog = document.getElementById('chatLog');
    const sendBtn = document.getElementById('sendBtn');
    const statusMessage = document.getElementById('statusMessage');
    const fileInfo = document.getElementById('fileInfo');
    const warning = document.getElementById('warning');

    // --- Environment Detection ---
    const isFileProtocol = window.location.protocol === 'file:';
    const generatorModel = isFileProtocol
      ? "Xenova/LaMini-Flan-T5-77M"   // fallback small model
      : "Xenova/LaMini-Flan-T5-248M"; // better model when hosted

    if (isFileProtocol) {
      warning.style.display = "block";
      warning.textContent = "⚠️ Running from file:// — Using smaller 77M model. For better results, host this file on localhost or GitHub Pages.";
    }

    // --- Utility ---
    function setStatus(message) {
      if (statusMessage) statusMessage.textContent = message;
    }

    function appendMessage(sender, text) {
      const div = document.createElement('div');
      div.className = sender;
      div.textContent = (sender === 'user' ? "You: " : "Bot: ") + text;
      chatLog.appendChild(div);
      chatLog.scrollTop = chatLog.scrollHeight;
    }

    function dot(a, b) {
      if (a.length !== b.length) throw new Error("Vector size mismatch");
      return a.reduce((sum, val, i) => sum + val * b[i], 0);
    }

    // --- Chunking ---
    function chunkText(text, chunkSize = 400, chunkOverlap = 100) {
      const chunks = [];
      let i = 0;
      while (i < text.length) {
        const end = Math.min(i + chunkSize, text.length);
        const chunk = text.substring(i, end).trim();
        if (chunk.length > 0) {
          chunks.push(chunk);
        }
        i += chunkSize - chunkOverlap;
      }
      return chunks.map((content, id) => ({ id, content }));
    }

    // --- Model Loading ---
    async function loadModels() {
      try {
        setStatus('1/2: Loading semantic search model...');
        state.fe = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');

        setStatus(`2/2: Loading generator model (${generatorModel})...`);
        state.generator = await pipeline('text2text-generation', generatorModel, { quantized: true });

        state.isReady = true;
        setStatus('✅ Models loaded. Using default demo document.');
        loadDefaultDoc();
      } catch (err) {
        console.error('Error loading models:', err);
        setStatus('❌ Failed to load models. Try running over localhost or GitHub Pages.');
      }
    }
    loadModels();

    // --- Sanitization ---
    function sanitize(text) {
      return String(text || '').replace(/\s+/g, ' ').trim();
    }

    // --- Load Default Document ---
    async function loadDefaultDoc() {
      state.chunks = chunkText(defaultDoc, 400, 100);

      state.embeddings = [];
      for (let chunk of state.chunks) {
        const emb = await state.fe(chunk.content, { pooling: 'mean', normalize: true });
        state.embeddings.push(Array.from(emb.data));
      }

      fileInfo.textContent = "Loaded: Demo JavaScript AI Doc";
      setStatus("✅ Document ready. Ask your question below.");
      userInput.disabled = false;
      sendBtn.disabled = false;
      userInput.focus();
    }

    // --- File Upload Handler ---
    fileInput.addEventListener('change', async (e) => {
      const file = e.target.files[0];
      if (!file) return;

      setStatus("Reading file...");
      const text = await file.text();
      fileInfo.textContent = `Loaded: ${file.name} (${text.length} chars)`;

      setStatus("1/2: Chunking document...");
      state.chunks = chunkText(text, 400, 100);

      setStatus(`2/2: Generating embeddings for ${state.chunks.length} chunks...`);
      state.embeddings = [];
      for (let chunk of state.chunks) {
        const emb = await state.fe(chunk.content, { pooling: 'mean', normalize: true });
        state.embeddings.push(Array.from(emb.data));
      }

      setStatus("✅ Document ready. Ask your question below.");
      userInput.disabled = false;
      sendBtn.disabled = false;
      userInput.focus();
    });

    // --- Main RAG Chat Handler ---
    async function handleSend() {
      const query = userInput.value.trim();
      if (!query) return;
      appendMessage('user', query);
      userInput.value = '';

      if (state.chunks.length === 0) {
        appendMessage('bot', "Please upload a document first.");
        return;
      }

      setStatus("1/3: Retrieving context...");
      let context = "";

      if (/summary|summarize|overview/i.test(query)) {
        context = state.chunks.slice(0, 5).map(c => c.content).join('\n\n');
      } else {
        const qEmb = await state.fe(query, { pooling: 'mean', normalize: true });
        const scoredChunks = state.embeddings.map((emb, i) => ({
          content: state.chunks[i].content,
          score: dot(Array.from(qEmb.data), emb),
        }));

        const TOP_K = 1;
        const topChunks = scoredChunks.sort((a, b) => b.score - a.score).slice(0, TOP_K);
        context = topChunks.map(c => c.content).join('\n\n');
      }

      const prompt = `
Use only the context below to answer the question.
Do not repeat the question. Do not invent information.

Context:
${context}

Question: ${query}

Answer in 2–5 sentences, clear and concise:
`.trim();

      setStatus("2/3: Generating answer...");
      const gen = await state.generator(prompt, {
        max_new_tokens: 200,
        temperature: 0.3,
        top_p: 0.9,
        do_sample: true,
      });

      setStatus("3/3: Finalizing...");
      let finalAnswer = sanitize(gen[0]?.generated_text || '');

      if (!finalAnswer) {
        finalAnswer = "I couldn't find a clear answer in the document for your question.";
      }

      appendMessage('bot', finalAnswer);
      setStatus("✅ Document ready. Ask another question.");
    }

    sendBtn.addEventListener('click', handleSend);
    userInput.addEventListener('keydown', (e) => {
      if (e.key === 'Enter') handleSend();
    });
  </script>
</body>
</html>
